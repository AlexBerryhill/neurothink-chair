{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d75cbe",
   "metadata": {},
   "source": [
    "# Eyes Open/Closed Detection\n",
    "\n",
    "In this notebook we will train a model to determine whether a person's eyes are open or closed based on EEG signals from the Muse 2 headset.\n",
    "\n",
    "## Running a Survey\n",
    "\n",
    "First we can import our library and run a survey, so we can train a model on the resulting data. We'll ask the participant to first get into a comfortable position, then open eyes for 30s, close for 30s, etc. 3x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8326ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "# Reload external source files when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"../src\")\n",
    "from recorder import Muse2EEGRecorder\n",
    "from survey import Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a901c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "    Error 305 for command:\n",
      "        open \"..\\data\\Meditation-bell-sound.mp3\"\n",
      "    Cannot specify extra characters after a string enclosed in quotation marks.\n",
      "\n",
      "    Error 305 for command:\n",
      "        close \"..\\data\\Meditation-bell-sound.mp3\"\n",
      "    Cannot specify extra characters after a string enclosed in quotation marks.\n",
      "Failed to close the file: \"..\\data\\Meditation-bell-sound.mp3\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next step -- Just breathe normally, gently relax any tension, get in a comfortable position.\n"
     ]
    },
    {
     "ename": "PlaysoundException",
     "evalue": "\n    Error 305 for command:\n        open \"..\\data\\Meditation-bell-sound.mp3\"\n    Cannot specify extra characters after a string enclosed in quotation marks.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPlaysoundException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amsb9\\Documents\\GitHub\\neurothink-chair\\EEG\\Muse-EEG-eyes-open.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amsb9/Documents/GitHub/neurothink-chair/EEG/Muse-EEG-eyes-open.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m muse2_recorder \u001b[39m=\u001b[39m Muse2EEGRecorder()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amsb9/Documents/GitHub/neurothink-chair/EEG/Muse-EEG-eyes-open.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m eyes_survey \u001b[39m=\u001b[39m Survey(muse2_recorder, \u001b[39m\"\u001b[39m\u001b[39mEyes open-closed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mEyes open for 30, closed for 30 - repeat 3x.\u001b[39m\u001b[39m\"\u001b[39m, eyes_schedule)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/amsb9/Documents/GitHub/neurothink-chair/EEG/Muse-EEG-eyes-open.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m eyes_survey\u001b[39m.\u001b[39;49mrecord(\u001b[39m\"\u001b[39;49m\u001b[39mAlex\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\amsb9\\Documents\\GitHub\\neurothink-chair\\EEG\\../src\\survey.py:52\u001b[0m, in \u001b[0;36mSurvey.record\u001b[1;34m(self, subject)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m# Play audio to signal next step\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_step_audio:\n\u001b[1;32m---> 52\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_play_next_step_audio(description)\n\u001b[0;32m     54\u001b[0m \u001b[39m# Execute custom handler function\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandler_thread\u001b[39m.\u001b[39msubmit(handler)\n",
      "File \u001b[1;32mc:\\Users\\amsb9\\Documents\\GitHub\\neurothink-chair\\EEG\\../src\\survey.py:174\u001b[0m, in \u001b[0;36mSurvey._play_next_step_audio\u001b[1;34m(self, description)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNext step -- \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m description)\n\u001b[0;32m    173\u001b[0m filename\u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:/Users/amsb9/Documents/GitHub/neurothink-chair/data/Meditation-bell-sound.mp3\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 174\u001b[0m ps\u001b[39m.\u001b[39;49mplaysound(\u001b[39m'\u001b[39;49m\u001b[39mMeditation-bell-sound.mp3\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    175\u001b[0m exec_cmd(\u001b[39m\"\u001b[39m\u001b[39mmpg123 ../data/Meditation-bell-sound.mp3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    176\u001b[0m exec_cmd(\u001b[39m\"\u001b[39m\u001b[39mbash -c \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmimic \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m description \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m -o EEG-Muse_tmp.wav && cvlc EEG-Muse_tmp.wav --play-and-exit && rm EEG-Muse_tmp.wav\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\amsb9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\playsound.py:72\u001b[0m, in \u001b[0;36m_playsoundWin\u001b[1;34m(sound, block)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mStarting\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m     winCommand(\u001b[39mu\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mopen \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(sound))\n\u001b[0;32m     73\u001b[0m     winCommand(\u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39mplay \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(sound, \u001b[39m'\u001b[39m\u001b[39m wait\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m block \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     74\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mReturning\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\amsb9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\playsound.py:64\u001b[0m, in \u001b[0;36m_playsoundWin.<locals>.winCommand\u001b[1;34m(*command)\u001b[0m\n\u001b[0;32m     60\u001b[0m     exceptionMessage \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m    Error \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(errorCode) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m for command:\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     61\u001b[0m                         \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m        \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m command\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-16\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m\n\u001b[0;32m     62\u001b[0m                         \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m    \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m errorBuffer\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-16\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mrstrip(\u001b[39m'\u001b[39m\u001b[39m\\0\u001b[39;00m\u001b[39m'\u001b[39m))\n\u001b[0;32m     63\u001b[0m     logger\u001b[39m.\u001b[39merror(exceptionMessage)\n\u001b[1;32m---> 64\u001b[0m     \u001b[39mraise\u001b[39;00m PlaysoundException(exceptionMessage)\n\u001b[0;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m buf\u001b[39m.\u001b[39mvalue\n",
      "\u001b[1;31mPlaysoundException\u001b[0m: \n    Error 305 for command:\n        open \"..\\data\\Meditation-bell-sound.mp3\"\n    Cannot specify extra characters after a string enclosed in quotation marks."
     ]
    }
   ],
   "source": [
    "eyes_open_step = (timedelta(seconds=30), \"eyes_open\", \"Please open your eyes.\", True)\n",
    "eyes_closed_step = (timedelta(seconds=30), \"eyes_closed\", \"Please close your eyes.\", True)\n",
    "eyes_schedule = [\n",
    "    (timedelta(seconds=30), \"intro\", \"Just breathe normally, gently relax any tension, get in a comfortable position.\", False),\n",
    "    eyes_open_step,\n",
    "    eyes_closed_step,\n",
    "    eyes_open_step,\n",
    "    eyes_closed_step,\n",
    "    eyes_open_step,\n",
    "    eyes_closed_step\n",
    "]\n",
    "\n",
    "test_schedule = [\n",
    "    (timedelta(seconds=5), \"intro\", \"Just breathe normally, gently relax any tension, get in a comfortable position.\", True)\n",
    "]\n",
    "\n",
    "muse2_recorder = Muse2EEGRecorder()\n",
    "eyes_survey = Survey(muse2_recorder, \"Eyes open-closed\", \"Eyes open for 30, closed for 30 - repeat 3x.\", eyes_schedule)\n",
    "eyes_survey.record(\"Alex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde1e5e4",
   "metadata": {},
   "source": [
    "## Preparing Data for Learning\n",
    "\n",
    "We need to transform our raw survey data into a format suitable for supervised learning. We will create an input tensor with the shape required by PyTorch - `(batch_size, kernel_size, seq_len)`, aka `(Samples, Variables, Length / time or sequence steps)`, or `[batch_size, channels, num_features (aka: H * W)]`.\n",
    "\n",
    "1. Batch size can be tuned. We will start with `64`.\n",
    "2. The second index is the number of features per batch. In this case, we have four EEG sensors, so that will be `4`.\n",
    "3. The number of samples included for each feature in each batch. This must be the same for every batch, so we will need to use a size <= the size of the smallest dataset we will use.\n",
    "\n",
    "The PyTorch `DataLoader` is an alternative to batching out data manually. After creating a `Dataset`, the DataLoader will batch data with a given batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a63694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from eeg_preprocessing import preprocess_eeg_channel\n",
    "from eegdata import EEGSurveyDataset, ChunkedDataset, MultiDataset\n",
    "\n",
    "def transform_normalize(data):\n",
    "    for ch in range(data.shape[1]):\n",
    "        data[:, ch] = preprocess_eeg_channel(data[:, ch])\n",
    "        stddev = data[:, ch].std()\n",
    "        if stddev != 0:\n",
    "            data[:, ch] = (data[:, ch] - data[:, ch].mean()) / stddev\n",
    "    return data\n",
    "\n",
    "batch_size = 35\n",
    "\n",
    "# Create PyTorch Datasets\n",
    "# For 30s datasets, the # of samples is between 7666 and 7936, with 7936 being the most common.\n",
    "ds1 = EEGSurveyDataset(\"../data\\muse2-recordings\\surveys\\Eyes open-closedAlex2023-02-28 175124.152478\", 7665, transform=transform_normalize)\n",
    "ds2 = EEGSurveyDataset(\"../data\\muse2-recordings\\surveys\\Eyes open-closed Alex 2023-02-28 180223.542443\", 7665, transform=transform_normalize)\n",
    "ds3 = EEGSurveyDataset(\"../data\\muse2-recordings\\surveys\\Eyes open-closed Alex 2023-02-28 181123.180577\", 7665, transform=transform_normalize)\n",
    "ds4 = EEGSurveyDataset(\"../data\\muse2-recordings\\surveys\\Eyes open-closed Alex 2023-02-28 182051.315931\", 7665, transform=transform_normalize)\n",
    "ds5 = EEGSurveyDataset(\"../data\\muse2-recordings\\surveys\\Eyes open-closed Alex 2023-03-01 190129.939632\", 7665, transform=transform_normalize)\n",
    "ds6 = EEGSurveyDataset(\"../data\\muse2-recordings\\surveys\\Eyes open-closed Alex 2023-03-01 190647.597127\", 7665, transform=transform_normalize)\n",
    "ds7 = EEGSurveyDataset(\"../data\\muse2-recordings\\surveys\\Eyes open-closed Alex 2023-03-01 191102.296501\", 7665, transform=transform_normalize)\n",
    "ds8 = EEGSurveyDataset(\"../data\\muse2-recordings\\surveys\\Eyes open-closed Alex 2023-03-01 191541.164491\", 7665, transform=transform_normalize)\n",
    "ds9 = EEGSurveyDataset(\"../data\\muse2-recordings\\surveys\\Eyes open-closed Alex 2023-03-01 192625.791140\", 7665, transform=transform_normalize)\n",
    "ds1, ds2, ds3, ds4, ds5, ds6, ds7, ds8 = ChunkedDataset(ds1, batch_size), ChunkedDataset(ds2, batch_size), ChunkedDataset(ds3, batch_size), ChunkedDataset(ds4, batch_size), ChunkedDataset(ds5, batch_size), ChunkedDataset(ds6, batch_size), ChunkedDataset(ds7, batch_size), ChunkedDataset(ds8, batch_size)\n",
    "train_dataset = MultiDataset([ds1, ds2, ds3, ds5, ds6, ds7])\n",
    "test_dataset = MultiDataset([ds4, ds8])\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284483e",
   "metadata": {},
   "source": [
    "## Evaluate Data Quality\n",
    "\n",
    "Some"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f2c95",
   "metadata": {},
   "source": [
    "## PyTorch Model\n",
    "\n",
    "Now we can create the neural network we will be training on the collected data. We will use the [simple keras model by Sentdex](https://github.com/Sentdex/BCI) as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ab1e1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input shape: torch.Size([35, 4, 219])\n",
      "Model output shape: torch.Size([35, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amsb9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_channels = 4\n",
    "n_outputs = 2\n",
    "\n",
    "# Create model\n",
    "net = nn.Sequential(\n",
    "    # Pass input to a 1D convolutional layer with a kernel size of 3, apply to activation function.\n",
    "    nn.Conv1d(n_channels, 32, 3),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Pass previous layer output to a 1D convolutional layer with a kernel size of 2, apply to activation function,\n",
    "    # and get the max value from each kernel.\n",
    "    nn.Conv1d(32, 32, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2),\n",
    "\n",
    "    # Pass previous layer output to a 1D convolutional layer with a kernel size of 2, apply to activation function,\n",
    "    # and get the max value from each kernel. (same as previous layer)\n",
    "    nn.Conv1d(32, 32, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2),\n",
    "\n",
    "    # Flatten the convolutions. Input shape: (a, b, c), Output shape: (a, b*c)\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    #nn.Dropout(0.5),\n",
    "    # ?\n",
    "    # XXX: The first number needs to be updated each time the input shapes change. We could instead\n",
    "    #      Create a class-based Module, and do a single pass through the conv portion of the network\n",
    "    #      in order to determine the actual size.\n",
    "    #      (This technique is shown in https://www.youtube.com/watch?v=1gQR24B3ISE&list=PLQVvvaa0QuDdeMyHEYc0gxFpYwHY2Qfdh&index=7).\n",
    "    #      For now, we can update this value as needed by commenting out all layers after Flatten(), then running the code\n",
    "    #      below and inspecting the output shape. The x[1] value should be the first arg in the following line.\n",
    "    nn.Linear(1696, 512),  # ~= nn.LazyLinear(512)\n",
    "\n",
    "    # Flatten the linear layer into the required number of outputs\n",
    "    nn.Linear(512, n_outputs),\n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    features, labels = data\n",
    "    print(\"Model input shape:\", features.shape)\n",
    "    out = net(features.float())\n",
    "    print(\"Model output shape:\", out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a72cd8",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b5b0ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [35:38<00:00, 26.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passes per epoch: 36.0\n",
      "Final Loss: 0.3704300820827484\n",
      "Final Accuracy: 65%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Train for n epochs\n",
    "n = 80\n",
    "test_while_training = True\n",
    "\n",
    "loss_history = []\n",
    "eval_history = []\n",
    "count = 0\n",
    "net.train()\n",
    "for epoch in tqdm(range(n)):\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        features, labels = data\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        #net.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        predictions = net(features.float())\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, labels.long())\n",
    "        loss_history.append(loss.item())\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    # Evaluate the model against the test dataset\n",
    "    if test_while_training:\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_dataloader, 0):\n",
    "                features, labels = data\n",
    "                out = net(features.float())\n",
    "                preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (preds == labels).sum().item()\n",
    "\n",
    "        eval_history.append(correct / total)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.title(\"Training session\")\n",
    "print(\"Passes per epoch:\", count / n)\n",
    "print(\"Final Loss:\", loss_history[-1])\n",
    "plt.plot(np.linspace(0, 1, len(loss_history)), loss_history, label=\"Loss\")\n",
    "if test_while_training:\n",
    "    print(f\"Final Accuracy: {int(100*eval_history[-1])}%\")\n",
    "    plt.plot(np.linspace(0, 1, len(eval_history)), eval_history, label=\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20128bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 273 / 420 - 65%\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader, 0):\n",
    "        features, labels = data\n",
    "        out = net(features.float())\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "model_accuracy = int(correct / total * 100)\n",
    "print(\"Correct:\", correct, \"/\", total, \"-\", f\"{model_accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cfa73f",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4cc6c12",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "File ../models/2023-03-02 09:35:45.025256-Muse_EEG_eyes_open-65percent.pt cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amsb9\\Documents\\GitHub\\neurothink-chair\\EEG\\Muse-EEG-eyes-open.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amsb9/Documents/GitHub/neurothink-chair/EEG/Muse-EEG-eyes-open.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amsb9/Documents/GitHub/neurothink-chair/EEG/Muse-EEG-eyes-open.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m timestamp \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(datetime\u001b[39m.\u001b[39mnow())\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/amsb9/Documents/GitHub/neurothink-chair/EEG/Muse-EEG-eyes-open.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m torch\u001b[39m.\u001b[39;49msave(net\u001b[39m.\u001b[39;49mstate_dict(), \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../models/\u001b[39;49m\u001b[39m{\u001b[39;49;00mtimestamp\u001b[39m}\u001b[39;49;00m\u001b[39m-Muse_EEG_eyes_open-\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_accuracy\u001b[39m}\u001b[39;49;00m\u001b[39mpercent.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amsb9/Documents/GitHub/neurothink-chair/EEG/Muse-EEG-eyes-open.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../models/\u001b[39m\u001b[39m{\u001b[39;00mtimestamp\u001b[39m}\u001b[39;00m\u001b[39m-Muse_EEG_eyes_open-\u001b[39m\u001b[39m{\u001b[39;00mmodel_accuracy\u001b[39m}\u001b[39;00m\u001b[39mpercent.model\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amsb9/Documents/GitHub/neurothink-chair/EEG/Muse-EEG-eyes-open.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     f\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(net))\n",
      "File \u001b[1;32mc:\\Users\\amsb9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:422\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    419\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m    421\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 422\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    424\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amsb9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:309\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[1;32mc:\\Users\\amsb9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:287\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 287\u001b[0m     \u001b[39msuper\u001b[39m(_open_zipfile_writer_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File ../models/2023-03-02 09:35:45.025256-Muse_EEG_eyes_open-65percent.pt cannot be opened."
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = str(datetime.now())\n",
    "torch.save(net.state_dict(), f\"../models/{timestamp}-Muse_EEG_eyes_open-{model_accuracy}percent.pt\")\n",
    "with open(f\"../models/{timestamp}-Muse_EEG_eyes_open-{model_accuracy}percent.model\", \"w\") as f:\n",
    "    f.write(str(net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c2fa6",
   "metadata": {},
   "source": [
    "## Raw EEG + FFT Model\n",
    "\n",
    "While our previous model had an impressive accuracy given the size of our dataset, it can probably be improved by adding more features. We will provide outputs from a Fast Fourier Transform as additional features, banded over the primary brainwave bands - `Alpha`, `Beta`, `Theta`, `Delta`, `Gamma` (possibly split into `HighGamma` and `LowGamma`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a98d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66753322",
   "metadata": {},
   "source": [
    "## Live Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1298d5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to muse device\n",
      "<pylsl.pylsl.XMLElement object at 0x000001C6E57D64A0>\n",
      "Muse 2 Sample rate: 256\n",
      "Muse 2 time correction: -2.214987762272358e-05\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1792 and 1696x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amsb9\\Documents\\GitHub\\neurothink-chair\\EEG\\Muse-EEG-eyes-open.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amsb9/Documents/GitHub/neurothink-chair/EEG/Muse-EEG-eyes-open.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m muse2_recorder\u001b[39m.\u001b[39mstream(timedelta(seconds\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amsb9/Documents/GitHub/neurothink-chair/EEG/Muse-EEG-eyes-open.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     samples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(samples[:, :\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mT, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/amsb9/Documents/GitHub/neurothink-chair/EEG/Muse-EEG-eyes-open.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     net(torch\u001b[39m.\u001b[39;49mfrom_numpy(samples)\u001b[39m.\u001b[39;49mfloat())\n",
      "File \u001b[1;32mc:\\Users\\amsb9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amsb9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\amsb9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amsb9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1792 and 1696x512)"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "muse2_recorder = Muse2EEGRecorder()\n",
    "muse2_recorder.connect()\n",
    "with torch.no_grad():\n",
    "    for samples in muse2_recorder.stream(timedelta(seconds=5)):\n",
    "        samples = np.expand_dims(samples[:, :4].T, axis=0)\n",
    "        net(torch.from_numpy(samples).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0463713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bce616fd2e2d4d39ebdd58440dc995d47c41b3773e58c13e9ec8c164e6c647aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
