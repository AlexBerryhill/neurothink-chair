{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d75cbe",
   "metadata": {},
   "source": [
    "# Muse EEG\n",
    "\n",
    "In this notebook we will train a model to associate EEG signals from the Muse 2 headset with the wearer's eyes being open or closed.\n",
    "\n",
    "## Running a Survey\n",
    "\n",
    "First we can import our library and create a survey, so we can train a model on the resulting data. We'll ask the participant to first get into a comfortable position, then open eyes for 30s, close for 30s, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a901c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# Reload external source files when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"../src\")\n",
    "from recorder import Muse2EEGRecorder\n",
    "from survey import Survey\n",
    "\n",
    "eyes_open_step = (timedelta(seconds=30), \"eyes_open\", \"Please open your eyes.\", True)\n",
    "eyes_closed_step = (timedelta(seconds=30), \"eyes_closed\", \"Please close your eyes.\", True)\n",
    "eyes_schedule = [\n",
    "    (timedelta(seconds=30), \"intro\", \"Just breathe normally, gently relax any tension, get in a comfortable position.\", False),\n",
    "    eyes_open_step,\n",
    "    eyes_closed_step,\n",
    "    eyes_open_step,\n",
    "    eyes_closed_step,\n",
    "    eyes_open_step,\n",
    "    eyes_closed_step\n",
    "]\n",
    "\n",
    "test_schedule = [\n",
    "    (timedelta(seconds=5), \"intro\", \"Just breathe normally, gently relax any tension, get in a comfortable position.\", True)\n",
    "]\n",
    "\n",
    "#muse2_recorder = Muse2EEGRecorder()\n",
    "#eyes_survey = Survey(muse2_recorder, \"Eyes open-closed\", \"Eyes open for 30, closed for 30 - repeat 3x.\", eyes_schedule)\n",
    "#eyes_survey.record(\"Jared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde1e5e4",
   "metadata": {},
   "source": [
    "## Preparing Data for Learning\n",
    "\n",
    "We need to transform our raw survey data into a format suitable for supervised learning. We will create an input tensor with the shape required by PyTorch - `(batch_size, kernel_size, seq_len)`, aka `(Samples, Variables, Length / time or sequence steps)`, or `[batch_size, channels, num_features (aka: H * W)]`.\n",
    "\n",
    "1. Batch size can be tuned. We will start with `64`.\n",
    "2. The second index is the number of features per batch. In this case, we have four EEG sensors, so that will be `4`.\n",
    "3. The number of samples included for each feature in each batch.\n",
    "\n",
    "The PyTorch `DataLoader` is an alternative to batching out data manually. After creating a `Dataset`, the DataLoader will batch data with a given batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a63694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eyes closed shape: (7936, 7)\n",
      "Eyes open shape: (7936, 7)\n",
      "Input features shape: (15872, 4)\n",
      "Input labels shape: (15872, 2)\n",
      "Training data shape (features, labels): ((11904, 4), (11904, 2))\n",
      "{'eyes_open': 0, 'eyes_closed': 1}\n",
      "{'eyes_open': 0, 'eyes_closed': 1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load EEG CSV\n",
    "eyes_closed_1 = pd.read_csv(\"../data/muse2-recordings/surveys/Eyes open-closed 2021-06-24 13:13:38.263066/2_eyes_closed-eeg_raw.csv\")\n",
    "eyes_open_1 = pd.read_csv(\"../data/muse2-recordings/surveys/Eyes open-closed 2021-06-24 13:13:38.263066/3_eyes_open-eeg_raw.csv\")\n",
    "print(\"Eyes closed shape:\", eyes_closed_1.shape)\n",
    "print(\"Eyes open shape:\", eyes_open_1.shape)\n",
    "\n",
    "# Create features ndarray\n",
    "closed_x = eyes_closed_1[[\"eeg1\", \"eeg2\", \"eeg3\", \"eeg4\"]].to_numpy()\n",
    "open_x = eyes_open_1[[\"eeg1\", \"eeg2\", \"eeg3\", \"eeg4\"]].to_numpy()\n",
    "X = np.concatenate((closed_x, open_x))\n",
    "print(\"Input features shape:\", X.shape)\n",
    "# TODO Scale data??\n",
    "\n",
    "# Create one-hot labels ndarray\n",
    "Y = np.vstack((\n",
    "    # Eyes are open column\n",
    "    np.concatenate((np.zeros((eyes_closed_1.shape[0])), np.ones((eyes_open_1.shape[0])))),\n",
    "    # Eyes are closed column\n",
    "    np.concatenate((np.ones((eyes_open_1.shape[0])), np.zeros((eyes_closed_1.shape[0]))))\n",
    ")).T\n",
    "print(\"Input labels shape:\", Y.shape)\n",
    "\n",
    "# Split into train, test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "print(\"Training data shape (features, labels):\", (X_train.shape, Y_train.shape))\n",
    "\n",
    "# Define a generic Dataset subclass\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data, labels, num_features, transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.num_features = num_features\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.labels) / self.num_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_i = int(idx * self.num_features)\n",
    "        end_i = start_i + self.num_features\n",
    "        datum = self.data[start_i:end_i]\n",
    "        #label = self.labels[start_i:end_i]\n",
    "        label = self.labels[end_i]\n",
    "        if self.transform:\n",
    "            datum = self.transform(datum)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return datum.T, label\n",
    "    \n",
    "class EEGSurveyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Given a survey path, load each eeg_raw csv file and load it as a datum.\n",
    "    \"\"\"\n",
    "    def __init__(self, survey_path, max_size, transform=None, target_transform=None):\n",
    "        self.data_files = [survey_path + \"/\" + f for f in os.listdir(survey_path) if f.endswith(\"eeg_raw.csv\")]\n",
    "        self.label_map = {f:self._parse_filename(f)[1] for f in self.data_files}\n",
    "        self.ilabel_map = self._create_ilabel_map()\n",
    "        print(self.ilabel_map)\n",
    "        self.max_size = max_size\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_file = self.data_files[idx]\n",
    "        data = pd.read_csv(data_file)[[\"eeg1\", \"eeg2\", \"eeg3\", \"eeg4\"]].to_numpy()[:self.max_size]\n",
    "        label = self.ilabel_map[self.label_map[data_file]]\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return data.T, label\n",
    "    \n",
    "    def _create_ilabel_map(self):\n",
    "        ilabel_map = {}\n",
    "        for key in self.label_map.keys():\n",
    "            if key not in ilabel_map:\n",
    "                ilabel_map[self.label_map[key]] = len(ilabel_map.keys())-1\n",
    "        return ilabel_map\n",
    "    \n",
    "    def _parse_filename(self, filename):\n",
    "        filename = filename.split(\"/\")[-1]\n",
    "        name, extension = filename.split(\".\", 1)\n",
    "        num_tag, typ = name.split(\"-\", 1)\n",
    "        num, tag = num_tag.split(\"_\", 1)\n",
    "        return num, tag\n",
    "\n",
    "\"\"\"\n",
    "class EEGSurveyDataset2(Dataset):\n",
    "    '''\n",
    "    Given a survey path and a batch_size, load each eeg_raw csv file in the path, and\n",
    "    split it into batches of size `batch_size`.\n",
    "    '''\n",
    "    def __init__(self, survey_path, batch_size, transform=None, target_transform=None):\n",
    "        self.data_files = [survey_path + \"/\" + f for f in os.listdir(survey_path) if f.endswith(\"eeg_raw.csv\")]\n",
    "        self.label_map = {f:self._parse_filename(f)[1] for f in self.data_files}\n",
    "        self.ilabel_map = self._create_ilabel_map()\n",
    "        print(self.ilabel_map)\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_file = self.data_files[idx]\n",
    "        data = pd.read_csv(data_file)[[\"eeg1\", \"eeg2\", \"eeg3\", \"eeg4\"]].to_numpy() #[:self.max_size]\n",
    "        label = self.ilabel_map[self.label_map[data_file]]\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        num_samples = int(np.ceil(len(data) / self.batch_size))\n",
    "        print(data.T.reshape((self.batch_size, 4, num_samples)).shape)\n",
    "        return data.T.reshape((self.batch_size, 4, num_samples)), label\n",
    "    \n",
    "    def _create_ilabel_map(self):\n",
    "        ilabel_map = {}\n",
    "        for key in self.label_map.keys():\n",
    "            if key not in ilabel_map:\n",
    "                ilabel_map[self.label_map[key]] = len(ilabel_map.keys())-1\n",
    "        return ilabel_map\n",
    "    \n",
    "    def _parse_filename(self, filename):\n",
    "        filename = filename.split(\"/\")[-1]\n",
    "        name, extension = filename.split(\".\", 1)\n",
    "        num_tag, typ = name.split(\"-\", 1)\n",
    "        num, tag = num_tag.split(\"_\", 1)\n",
    "        return num, tag\n",
    "\"\"\"\n",
    "\n",
    "# Create PyTorch Datasets\n",
    "#train_dataset, test_dataset = EEGDataset(X_train, Y_train, 256), EEGDataset(X_test, Y_test, 256)\n",
    "train_dataset, test_dataset = EEGSurveyDataset(\"../data/muse2-recordings/surveys/Eyes open-closed Jared 2021-06-26 16:01:52.123715\", 256), EEGSurveyDataset(\"../data/muse2-recordings/surveys/Eyes open-closed Jared 2021-06-26 16:33:30.120561\", 7666)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f2c95",
   "metadata": {},
   "source": [
    "## PyTorch Model\n",
    "\n",
    "Now we can create the neural network we will be training on the collected data. We will model it after the [simple BCI model by Sentdex](https://github.com/Sentdex/NNfSiX). Sentdex doesn't provide the training or validation data, so we will assume the input is simply batches of raw EEG data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ab1e1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Model input shape: torch.Size([6, 4, 256])\n",
      "Model output shape: torch.Size([6, 2])\n",
      "tensor([[0.9774, 0.0226],\n",
      "        [0.4656, 0.5344],\n",
      "        [0.8724, 0.1276],\n",
      "        [0.3181, 0.6819],\n",
      "        [0.7758, 0.2242],\n",
      "        [0.1740, 0.8260]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "hidden_layers_size = 64\n",
    "n_channels = 4\n",
    "n_outputs = 2\n",
    "\n",
    "# Create model\n",
    "net = nn.Sequential(\n",
    "    # Pass input to a 1D convolutional layer with a kernel size of 3, apply to activation function.\n",
    "    nn.Conv1d(n_channels, hidden_layers_size, 3),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Pass previous layer output to a 1D convolutional layer with a kernel size of 2, apply to activation function,\n",
    "    # and get the max value from each kernel.\n",
    "    nn.Conv1d(hidden_layers_size, hidden_layers_size, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2),\n",
    "\n",
    "    # Pass previous layer output to a 1D convolutional layer with a kernel size of 2, apply to activation function,\n",
    "    # and get the max value from each kernel. (same as previous layer)\n",
    "    nn.Conv1d(hidden_layers_size, hidden_layers_size, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2),\n",
    "\n",
    "    # Flatten the convolutions. Input shape: (a, b, c), Output shape: (a, b*c)\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    # ?\n",
    "    # XXX: The first number needs to be updated each time the input shapes change. We could instead\n",
    "    #      Create a class-based Module, and do a single pass through the conv portion of the network\n",
    "    #      in order to determine the actual size.\n",
    "    #      (This technique is shown in https://www.youtube.com/watch?v=1gQR24B3ISE&list=PLQVvvaa0QuDdeMyHEYc0gxFpYwHY2Qfdh&index=7)\n",
    "    nn.Linear(3968, 512),\n",
    "    #nn.ReLU(),\n",
    "\n",
    "    # ?\n",
    "    nn.Linear(512, n_outputs),\n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "print(len(train_dataloader))\n",
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    features, labels = data\n",
    "    print(\"Model input shape:\", features.shape)\n",
    "    out = net(features.float())\n",
    "    print(\"Model output shape:\", out.shape)\n",
    "    print(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333d5dd",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04b5b0ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9151536822319031\n",
      "loss: 0.6465950012207031\n",
      "loss: 0.8132615685462952\n",
      "loss: 0.8132616877555847\n",
      "loss: 0.8132615685462952\n",
      "loss: 0.8132616877555847\n",
      "loss: 0.8132615685462952\n",
      "loss: 0.8132616877555847\n",
      "loss: 0.8132615685462952\n",
      "loss: 0.8132616877555847\n",
      "loss: 0.8132616877555847\n",
      "loss: 0.8132615685462952\n",
      "loss: 0.8132616877555847\n",
      "loss: 0.8132615685462952\n",
      "loss: 0.8132616877555847\n",
      "loss: 0.8132616877555847\n",
      "loss: 0.8132615685462952\n",
      "loss: 0.8132615685462952\n",
      "loss: 0.8132616877555847\n",
      "loss: 0.8132616877555847\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "# Train for n epochs\n",
    "n = 20\n",
    "net.train()\n",
    "for epoch in range(n):\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        features, labels = data\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        predictions = net(features.float())\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, labels.long())\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss\n",
    "        print(f\"loss: {loss.item()}\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f59d4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6]) torch.Size([6]) torch.Size([6, 2])\n",
      "Correct: 3 / 6\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader, 0):\n",
    "        features, labels = data\n",
    "        out = net(features.float())\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        print(labels.size(), preds.size(), out.size())\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "print(\"Correct:\", correct, \"/\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8b7ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738cda95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
